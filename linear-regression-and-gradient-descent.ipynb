{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\"Agg\")\nfrom matplotlib import pyplot as plt\nimport math as math","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:16:53.386435Z","iopub.execute_input":"2021-09-09T17:16:53.386752Z","iopub.status.idle":"2021-09-09T17:16:53.392047Z","shell.execute_reply.started":"2021-09-09T17:16:53.386723Z","shell.execute_reply":"2021-09-09T17:16:53.390878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daynight = {\n    \"D\" : 1,\n    \"N\" : 0\n}\ngroups = ['Low', 'Med', 'High', 'Exp']\ngroups_value = {\n    \"Low\": 3,\n    \"Med\": 5,\n    \"High\": 7,\n    \"Exp\":10\n}\nsatellite = {\n    \"Terra\": 5,\n    \"Aqua\": 10\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-09T18:02:55.635925Z","iopub.execute_input":"2021-09-09T18:02:55.636391Z","iopub.status.idle":"2021-09-09T18:02:55.640961Z","shell.execute_reply.started":"2021-09-09T18:02:55.636357Z","shell.execute_reply":"2021-09-09T18:02:55.640028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass BasisFunction:\n    def transform(self, data):\n        # 2n Degree basis\n        data['bright_track'] = data['brightness']*data['track']\n        data['bright_scan'] = data['brightness']*data['scan']\n        data['track_scan'] = data['track']*data['scan']\n        data['bright_bright_t31'] = data['brightness']*data['bright_t31']\n        data['daynight_brightness'] = data['daynight'] * data['brightness']\n        data['daynight_track'] = data['daynight'] * data['track']\n        data['daynight_scan'] = data['daynight'] * data['scan']\n        data['daynight_confidence'] = data['daynight'] * data['confidence']\n        data['brightness_2'] = data[\"brightness\"].apply(lambda x: x**2)\n        data['confidence'] = data[\"confidence\"].apply(lambda x: x**2)\n        data['bright_t31'] = data[\"bright_t31\"].apply(lambda x: x**.7)\n        data['scan_2'] = data[\"scan\"].apply(lambda x: x**.7)\n        data['track_2'] = data[\"track\"].apply(lambda x: x**.7)\n        data['daynight'] = data[\"daynight\"].apply(lambda x: x**2)\n        data['confidence_level_brightness'] = data['confidence_level'] * data['brightness']\n        data['confidence_level_daynight'] = data['confidence_level'] * data['daynight']\n        data['confidence_level_brightt31'] = data['confidence_level'] * data['bright_t31']\n        data['confidence_level_scan'] = data['confidence_level'] * data['scan']\n        data['acq_date_brightness'] = data['acq_date'] * data['brightness']\n        data['acq_date_daynight'] = data['acq_date'] * data['daynight']\n        \n\n#         # 3rd degree baisis of some feature\n#         data['track_2*confidence_2'] = (data['track']**2)*(data['confidence']**2)\n#         data['confidence_2*bright_t31_2']=(data['confidence']**2)*(data['bright_t31']**2)\n#         data['track_3*confidence_2'] = (data['track']**3)*(data['confidence']**2)\n#         data['track_3*brightness'] = (data['track']**3)*(data['brightness']**2)\n#         data['track_3*brightness_daynight'] = (data['track']**3)*(data['brightness']**2) * data['daynight']\n#         data['track_3*bright_t31_2_daynight'] = (data['track']**3)*(data['bright_t31']**2) * data['daynight']\n#         data['track_3*confidence_2_daynight'] = (data['track']**3)*(data['confidence']**2) * data['daynight']\n#         data['track_3*confidence_2_brightness'] = (data['track']**3)*(data['confidence']**2) * data['brightness']\n#         data['track_3*confidence_2_2'] = (data['track_2*confidence_2'])**2\n#         data['track_3*brightness_daynight_2'] = data['track_3*brightness_daynight']**2\n        return data \n#\n\nclass Scaler():\n    # hint: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n    def __init__(self):\n        self.min = {}\n        self.max = {}\n    def __call__(self, data, is_train=False):\n    #     normalization\n        if(is_train):\n            self.min = data.min()\n            self.max = data.max()\n        return (data-self.min)/(self.max-self.min)\n\ndef get_features(csv_path, is_train=False, scaler=None):\n    data = pd.read_csv(csv_path, usecols=['latitude', 'longitude', 'brightness', 'scan', \n                                          'track', 'acq_time', 'confidence', 'bright_t31', 'acq_date', 'daynight', 'satellite'])\n    data['daynight'] = data['daynight'].replace(daynight)\n    data['satellite'] = data['satellite'] .replace(satellite)\n    data['acq_date'] = pd.to_datetime(data['acq_date']) - pd.to_datetime(data['acq_date'].min())\n    data['acq_date'] = data['acq_date'].dt.days\n    data['confidence_level'] = pd.qcut(data['confidence'], q=4, labels=groups)\n    data['confidence_level'] = data['confidence_level'] .replace(groups_value)\n\n    #     #scaling value of mostly correlated field\n    basis = BasisFunction()\n    data = basis.transform(data)\n    \n    if scaler:\n        data = scaler.__call__(data, is_train)\n        \n    return data.to_numpy()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T18:03:02.747929Z","iopub.execute_input":"2021-09-09T18:03:02.748275Z","iopub.status.idle":"2021-09-09T18:03:02.762611Z","shell.execute_reply.started":"2021-09-09T18:03:02.748242Z","shell.execute_reply":"2021-09-09T18:03:02.761589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_targets(csv_path):\n    data = pd.read_csv(csv_path)['frp']\n    return data.to_numpy()\n\n\ndef analytical_solution(feature_matrix, targets, C=0.0):\n    transpose = np.transpose(feature_matrix)\n    tempmatrix = np.matmul(transpose,feature_matrix)-C*np.ones(len(transpose))\n    tempinverse = np.linalg.inv(tempmatrix)\n    temp2 = np.matmul(transpose,targets)\n    return np.matmul(tempinverse, temp2)\n\n\ndef do_evaluation(feature_matrix, targets, weights):\n    # your predictions will be evaluated based on mean squared error\n    predictions = get_predictions(feature_matrix, weights)\n    loss = mse_loss(feature_matrix, weights, targets)\n    return loss\n\ndef get_predictions(feature_matrix, weights):\n    return feature_matrix@weights\n\ndef mse_loss(feature_matrix, weights, targets):\n    return np.square(np.subtract(targets, get_predictions(feature_matrix,weights))).mean()\n\n\ndef l2_regularizer(weights):\n    return np.dot(weights, weights)\n\n\ndef loss_fn(feature_matrix, weights, targets, C=0.0):\n    return mse_loss(feature_matrix, weights, targets) + C * l2_regularizer(weights);\n\n\ndef compute_gradients(feature_matrix, weights, targets, C=0.0):\n    predicted = get_predictions(feature_matrix, weights)\n    array = initialize_weights(len(weights))\n    for k in range(len(weights)):\n        result = 0.0;\n        for i in range(len(feature_matrix)):\n            result = result + np.dot((predicted[i] - targets[i]),feature_matrix[i][k])\n        result = result + C * weights[k]\n        result=result/len(feature_matrix)\n        array[k]=result\n    return array\n                   \n    \ndef sample_random_batch(feature_matrix, targets, batch_size):\n    indices = np.random.choice(feature_matrix.shape[0], size = batch_size)\n    return (feature_matrix[indices, :], targets[indices])\n\n\ndef initialize_weights(n):\n    return np.zeros(n);\n\n\ndef update_weights(weights, gradients, lr):\n    weights = np.subtract(weights , np.dot(lr,gradients))\n    return weights\n\ndef early_stopping(min_not_change_count):\n    # allowed to modify argument list as per your need\n    # return True or False\n    return  min_not_change_count >= 500\n\n\ndef plot_trainsize_losses(x, y, xlabel, ylabel, figname, filename=\"test.jpg\"):\n    '''\n    Description:\n    plot losses on the development set instances as a function of training set size\n    '''\n\n    '''\n    Arguments:\n    # you are allowed to change the argument list any way you like \n    '''\n    fig = plt.figure()\n    plt.plot(x, y, color=\"blue\")\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    fig.suptitle(figname, fontsize=20)\n    plt.savefig(filename)\n    \n    \ndef do_gradient_descent(train_feature_matrix,\n                        train_targets,\n                        dev_feature_matrix,\n                        dev_targets,\n                        lr=1.0,\n                        C=0.0,\n                        batch_size=32,\n                        max_steps=50000,\n                        eval_steps=5):\n    '''\n    feel free to significantly modify the body of this function as per your needs.\n    ** However **, you ought to make use of compute_gradients and update_weights function defined above\n    return your best possible estimate of LR weights\n\n    a sample code is as follows --\n    '''\n    min_loss = np.inf\n    min_loss_unchange_count = 0\n    best_weight = initialize_weights(train_feature_matrix.shape[1])\n    weights = initialize_weights(train_feature_matrix.shape[1])\n    dev_loss = mse_loss(dev_feature_matrix, weights, dev_targets)\n    train_loss = mse_loss(train_feature_matrix, weights, train_targets)\n    print(\"step {} \\t dev loss: {} \\t train loss: {}\".format(0, dev_loss, train_loss))\n    for step in range(1, max_steps + 1):\n\n        # sample a batch of features and gradients\n        features, targets = sample_random_batch(train_feature_matrix, train_targets, batch_size)\n        \n        # compute gradients\n        gradients = compute_gradients(features, weights, targets, C)\n\n        # update weights\n        weights = update_weights(weights, gradients, lr)\n\n        if step % eval_steps == 0:\n            dev_loss = mse_loss(dev_feature_matrix, weights, dev_targets)\n            train_loss = mse_loss(train_feature_matrix, weights, train_targets)\n            print(\"step {} \\t dev loss: {} \\t train loss: {}\".format(step, dev_loss, train_loss))\n\n            '''\n            implement early stopping etc. to improve performance.\n            '''\n            if min_loss>dev_loss:\n                if(min_loss - dev_loss)>200:\n                    min_loss = dev_loss\n                    min_loss_unchange_count = 0\n                else:\n                    min_loss_unchange_count +=1\n                best_weight = weights\n            else:\n                min_loss_unchange_count +=1\n                if early_stopping(min_loss_unchange_count):\n                    print(min_loss)\n                    break\n        \n    return best_weight\n\n\nif __name__ == '__main__':\n\n    scaler = Scaler()\n    train_features, train_targets = get_features('../input/data-set/train.csv', True, scaler), get_targets('../input/data-set/train.csv')\n    dev_features, dev_targets = get_features('../input/data-set/dev.csv', False, scaler), get_targets('../input/data-set/dev.csv')\n    a_solution = analytical_solution(train_features, train_targets, C=1e-34)\n    print('evaluating analytical_solution...')\n    dev_loss = do_evaluation(dev_features, dev_targets, a_solution)\n    train_loss = do_evaluation(train_features, train_targets, a_solution)\n    print('analytical_solution \\t train loss: {}, dev_loss: {} '.format(train_loss, dev_loss))\n    test = get_features('../input/data-set/test.csv', True, scaler)\n    df = pd.DataFrame(get_predictions(test, a_solution))\n    df.to_csv(\"./data.csv\")\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T18:03:04.97789Z","iopub.execute_input":"2021-09-09T18:03:04.978223Z","iopub.status.idle":"2021-09-09T18:03:05.532014Z","shell.execute_reply.started":"2021-09-09T18:03:04.97819Z","shell.execute_reply":"2021-09-09T18:03:05.530681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = Scaler()\ntrain_features, train_targets = get_features('../input/data-set/train.csv', True, scaler), get_targets('../input/data-set/train.csv')\ndev_features, dev_targets = get_features('../input/data-set/dev.csv', False, scaler), get_targets('../input/data-set/dev.csv')\nprint('training LR using gradient descent...')\ngradient_descent_soln = do_gradient_descent(train_features,\n                                            train_targets,\n                                            dev_features,\n                                            dev_targets,\n                                            lr=0.25,\n                                            C=0,\n                                            batch_size=32,\n                                            max_steps=150000,\n                                            eval_steps=1000)\n\nprint('evaluating iterative_solution...')\ndev_loss = do_evaluation(dev_features, dev_targets, gradient_descent_soln)\ntrain_loss = do_evaluation(train_features, train_targets, gradient_descent_soln)\nprint('gradient_descent_soln \\t train loss: {}, dev_loss: {} '.format(train_loss, dev_loss))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T17:48:28.308528Z","iopub.execute_input":"2021-09-09T17:48:28.308879Z","iopub.status.idle":"2021-09-09T17:59:11.839345Z","shell.execute_reply.started":"2021-09-09T17:48:28.308844Z","shell.execute_reply":"2021-09-09T17:59:11.837059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=[]\ny=[]\nfor i in range(5000, train_features.shape[0], 5000):\n    a_solution = analytical_solution(train_features[:i], train_targets[:i], C=1e-8)\n    y.append(do_evaluation(dev_features, dev_targets, a_solution))\n    x.append(i)\nx.append(train_features.shape[0])\ny.append(dev_loss)\nplot_trainsize_losses(x,y,\"training data size\", \"MSE\", \n                          \"MSE with respect to training size\",\"analytical.jpg\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T11:45:11.259311Z","iopub.execute_input":"2021-09-09T11:45:11.259722Z","iopub.status.idle":"2021-09-09T11:45:11.457104Z","shell.execute_reply.started":"2021-09-09T11:45:11.259687Z","shell.execute_reply":"2021-09-09T11:45:11.455965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"C_values = []\nloss_values = []\nlamda_values = [ 0, 1e-8, 1e-6, 1e-5, 0.075, 0.1 , 0.25, 0.5, .75, 1]\nfor c in lamda_values:\n    gradient_descent_soln = do_gradient_descent(train_features, \n                        train_targets, \n                        dev_features,\n                        dev_targets,\n                        lr=0.5,\n                        C=c,\n                        batch_size=32,\n                        max_steps=50000,\n                        eval_steps=5)\n    dev_loss=do_evaluation(dev_features, dev_targets, gradient_descent_soln)\n    C_values.append(c)\n    loss_values.append(dev_loss)\n\nplot_trainsize_losses(C_values, loss_values, \"(C) value\", \"Dev Loss\",\"Lambda vs Dev Loss\",\"losswithC\")","metadata":{"execution":{"iopub.status.busy":"2021-09-09T12:01:14.477293Z","iopub.execute_input":"2021-09-09T12:01:14.477837Z","iopub.status.idle":"2021-09-09T12:36:37.963949Z","shell.execute_reply.started":"2021-09-09T12:01:14.4778Z","shell.execute_reply":"2021-09-09T12:36:37.962931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_trainsize_losses(C_values, loss_values, \"(C) value\", \"Dev Loss\",\"Lambda vs Dev Loss\",\"losswithC\")","metadata":{"execution":{"iopub.status.busy":"2021-09-09T12:43:05.228073Z","iopub.execute_input":"2021-09-09T12:43:05.228431Z","iopub.status.idle":"2021-09-09T12:43:05.337981Z","shell.execute_reply.started":"2021-09-09T12:43:05.2284Z","shell.execute_reply":"2021-09-09T12:43:05.336836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}